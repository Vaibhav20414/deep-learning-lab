ğŸ§  Deep Learning Lab

A hands-on repository for implementing fundamental deep learning concepts from scratch using PyTorch.

This lab focuses on understanding:
Tensor operations
Autograd and backpropagation
Linear & logistic regression
Feedforward neural networks
Dataset and DataLoader abstraction
Optimization algorithms

The goal is not just to use high-level APIs, but to deeply understand how models learn.

ğŸ“‚ Repository Structure
Deep-Learning-Lab/
â”‚
â”œâ”€â”€ linear_regression/
â”‚   â”œâ”€â”€ basic_gradient_descent.py
â”‚   â”œâ”€â”€ nn_module_version.py
â”‚
â”œâ”€â”€ logistic_regression/
â”‚   â”œâ”€â”€ binary_classification.py
â”‚   â”œâ”€â”€ xor_problem.py
â”‚
â”œâ”€â”€ multiple_linear_regression/
â”‚   â”œâ”€â”€ multi_feature_regression.py
â”‚
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ logistic_data.csv
â”‚   â”œâ”€â”€ mulRegData.csv
â”‚
â””â”€â”€ README.md 

This is yet to happen.

ğŸš€ Implemented Models
1ï¸âƒ£ Linear Regression

Manual gradient descent
Using nn.Module
Using Dataset and DataLoader
Mini-batch training
Loss visualization

2ï¸âƒ£ Logistic Regression

Binary classification
BCELoss vs BCEWithLogitsLoss
Proper train/test split
Decision boundary understanding

3ï¸âƒ£ XOR Neural Network

Demonstrates failure of logistic regression
One hidden layer neural network
Sigmoid and ReLU activation
Non-linear decision boundaries

4ï¸âƒ£ Multiple Linear Regression

Multi-feature input
Matrix multiplication (x @ w)
Parameter optimization
Loss curve plotting

ğŸ”¬ Core Concepts Practiced

Forward pass computation
Backpropagation with .backward()
Gradient accumulation & zeroing
Model modes: train() vs eval()
torch.no_grad() usage
Batch training via DataLoader
Non-linearity in neural networks
Why linear models fail on XOR

ğŸ›  Technologies Used

Python
PyTorch
Pandas
Matplotlib